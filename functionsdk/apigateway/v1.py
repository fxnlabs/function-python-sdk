# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: protobuf/apigateway.proto
# plugin: python-betterproto
from dataclasses import dataclass
from typing import AsyncGenerator, List

import betterproto
import grpclib


class ImageQuality(betterproto.Enum):
    """
    ImageQuality represents possible quality levels for image generation.
    """

    # Unspecified quality.
    IMAGE_QUALITY_UNSPECIFIED = 0
    # Standard quality.
    IMAGE_QUALITY_STANDARD = 1
    # HD quality.
    IMAGE_QUALITY_HD = 2


@dataclass
class ChatCompleteMessage(betterproto.Message):
    """Message format commonly used for chat completion."""

    # The role of the message sender.
    role: str = betterproto.string_field(1)
    # The content of the message.
    content: str = betterproto.string_field(2)


@dataclass
class ChatCompleteRequest(betterproto.Message):
    """
    ChatCompleteRequest represents the request for a single chat completion.
    """

    # The model to use for processing the message.
    model: str = betterproto.string_field(1)
    # The messages to be processed by the API.
    message: List["ChatCompleteMessage"] = betterproto.message_field(2)


@dataclass
class ChatCompleteResponse(betterproto.Message):
    """
    ChatCompleteResponse represents the response from the chat completion
    request.
    """

    # The completed response generated by the API.
    response: "ChatCompleteMessage" = betterproto.message_field(1)
    # The number of tokens in the response.
    token_count: int = betterproto.int32_field(2)


@dataclass
class ChatCompleteStreamRequest(betterproto.Message):
    """
    ChatCompleteStreamRequest represents the request for a stream of chat
    completions.
    """

    # The model to use for processing the message.
    model: str = betterproto.string_field(1)
    # The messages to be processed by the API.
    message: List["ChatCompleteMessage"] = betterproto.message_field(2)


@dataclass
class ChatCompleteStreamResponse(betterproto.Message):
    """
    ChatCompleteStreamResponse represents the response from a stream of chat
    completions.
    """

    # The completed response generated by the API. Either the role or the content
    # will be empty, but never both. The first instance of a stream response will
    # contain only the role, and the subsequent ones will contain only the
    # content.
    response: "ChatCompleteMessage" = betterproto.message_field(1)


@dataclass
class EmbedRequest(betterproto.Message):
    """
    EmbedRequest represents the request to generate embeddings for the provided
    input.
    """

    # The model to be used for generating embeddings.
    model: str = betterproto.string_field(1)
    # The input text to be embedded.
    input: str = betterproto.string_field(2)


@dataclass
class EmbedResponse(betterproto.Message):
    """
    EmbedResponse represents the response containing the embeddings and usage
    information.
    """

    # The type of the object returned.
    object: str = betterproto.string_field(1)
    # The list of data items containing embeddings.
    data: List["EmbedResponseData"] = betterproto.message_field(2)
    # The model used for generating the embeddings.
    model: str = betterproto.string_field(3)
    # Usage details for the request.
    usage: "EmbedResponseUsage" = betterproto.message_field(4)


@dataclass
class EmbedResponseUsage(betterproto.Message):
    """Usage provides information about the token usage for the request."""

    # The number of tokens used in the prompt.
    prompt_tokens: int = betterproto.int32_field(1)
    # The total number of tokens used (including prompt and response).
    total_tokens: int = betterproto.int32_field(2)


@dataclass
class EmbedResponseData(betterproto.Message):
    """Data provides the embedding details for the request."""

    # The type of the object returned.
    object: str = betterproto.string_field(1)
    # The list of embedding values.
    embedding: List[float] = betterproto.float_field(2)
    # The index of the data item in the response.
    index: int = betterproto.int32_field(3)


@dataclass
class TextToImageRequest(betterproto.Message):
    """
    ImageRequest represents the request to generate an image from a text
    prompt.
    """

    # The model to be used for generating the image.
    model: str = betterproto.string_field(1)
    # The prompt text to be used for generating the image.
    prompt: str = betterproto.string_field(2)
    # The total number of images to generate. Will default to 1 if not specified
    # or 0. May be subject to a limit.
    count: int = betterproto.uint32_field(3)
    # The quality of the image to generate.
    quality: "ImageQuality" = betterproto.enum_field(4)
    # The size of the image to generate. The format used is "<width>x<height>",
    # e.g. "1024x1024". Will default to "1024x1024" if not specified. Different
    # sizes and aspect ratios are supported by different models. Consult
    # documentation for the model you are using for guidance.
    size: str = betterproto.string_field(5)


@dataclass
class TextToImageResponse(betterproto.Message):
    """
    ImageResponse represents the response containing the generated image.
    """

    # All generated images.
    images: List["TextToImageResponseImage"] = betterproto.message_field(1)


@dataclass
class TextToImageResponseImage(betterproto.Message):
    # The URL of the generated image. Make a HEAD request to the URL to get
    # metadata about the image without downloading it.
    url: str = betterproto.string_field(1)
    # The Unix epoch second timestamp when the image URL will expire.
    expires_ts: int = betterproto.int64_field(2)


@dataclass
class TranscribeRequest(betterproto.Message):
    """TranscribeRequest represents the request to transcribe audio."""

    # The model to use for transcription.
    model: str = betterproto.string_field(1)
    # The URL to the audio file to transcribe. Most common audio formats are
    # supported (e.g. mp3, wav, ogg, flac). The URL is expected to end with a
    # file extension corresponding to the audio format. Note that some backends
    # may not support more than 25MB of audio data per request.
    url: str = betterproto.string_field(2)


@dataclass
class TranscribeResponse(betterproto.Message):
    """
    TranscribeResponse represents the response from the transcription API.
    """

    # The complete transcription of the audio file.
    text: str = betterproto.string_field(1)
    # The number of words in the transcription.
    word_count: int = betterproto.int32_field(2)
    # All words and their timestamps in the transcription.
    words: List["TranscribeResponseWord"] = betterproto.message_field(3)


@dataclass
class TranscribeResponseWord(betterproto.Message):
    # The word.
    word: str = betterproto.string_field(1)
    # The second (including fractional part) where the word starts.
    start_second: float = betterproto.double_field(2)
    # The second (including fractional part) where the word ends.
    end_second: float = betterproto.double_field(3)


class APIGatewayServiceStub(betterproto.ServiceStub):
    """
    APIGatewayService defines the service for interacting with the Function AI
    Services.
    """

    async def chat_complete(
        self, *, model: str = "", message: List["ChatCompleteMessage"] = []
    ) -> ChatCompleteResponse:
        """ChatComplete sends a message and returns a completion response."""

        request = ChatCompleteRequest()
        request.model = model
        if message is not None:
            request.message = message

        return await self._unary_unary(
            "/apigateway.v1.APIGatewayService/ChatComplete",
            request,
            ChatCompleteResponse,
        )

    async def chat_complete_stream(
        self, *, model: str = "", message: List["ChatCompleteMessage"] = []
    ) -> AsyncGenerator[ChatCompleteStreamResponse, None]:
        """
        ChatCompleteStream sends a message and receives a stream of completion
        responses. The first response will contain only the role. Subsequent
        responses will contain only the content. The stream will end without an
        error when the response is finished. The stream may return an error at
        any time if the response fails part-way through generation.
        """

        request = ChatCompleteStreamRequest()
        request.model = model
        if message is not None:
            request.message = message

        async for response in self._unary_stream(
            "/apigateway.v1.APIGatewayService/ChatCompleteStream",
            request,
            ChatCompleteStreamResponse,
        ):
            yield response

    async def embed(self, *, model: str = "", input: str = "") -> EmbedResponse:
        """
        Embed generates embeddings for the provided input based on the
        specified model.
        """

        request = EmbedRequest()
        request.model = model
        request.input = input

        return await self._unary_unary(
            "/apigateway.v1.APIGatewayService/Embed",
            request,
            EmbedResponse,
        )

    async def text_to_image(
        self,
        *,
        model: str = "",
        prompt: str = "",
        count: int = 0,
        quality: "ImageQuality" = 0,
        size: str = "",
    ) -> TextToImageResponse:
        """TextToImage generates an image from a text prompt."""

        request = TextToImageRequest()
        request.model = model
        request.prompt = prompt
        request.count = count
        request.quality = quality
        request.size = size

        return await self._unary_unary(
            "/apigateway.v1.APIGatewayService/TextToImage",
            request,
            TextToImageResponse,
        )

    async def transcribe(self, *, model: str = "", url: str = "") -> TranscribeResponse:
        """Transcribe transcribes an audio file."""

        request = TranscribeRequest()
        request.model = model
        request.url = url

        return await self._unary_unary(
            "/apigateway.v1.APIGatewayService/Transcribe",
            request,
            TranscribeResponse,
        )
